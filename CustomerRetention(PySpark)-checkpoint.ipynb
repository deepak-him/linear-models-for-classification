{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer retention case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/anaconda3/bin/python3.7'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/anaconda3/bin/python3.7'\n",
    "os.environ[\"SPARK_HOME\"] = '/opt/cloudera/parcels/CDH/lib/spark'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Customer retention').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+-----------+--------+------+---+------+-----------+-----------+---------+------------+---------+------+\n",
      "|SrNo|CustomerId|LastName|CreditScore|Location|   Sex|Age|Tenure|CurrBalance|NumProducts|OwnCrCard|ActiveMember|   Salary|Exited|\n",
      "+----+----------+--------+-----------+--------+------+---+------+-----------+-----------+---------+------------+---------+------+\n",
      "|   1|  15634602|Hargrave|        619|  France|Female| 42|     2|        0.0|          1|        1|           1|101348.88|     1|\n",
      "|   2|  15647311|    Hill|        608|   Spain|Female| 41|     1|   83807.86|          1|        0|           1|112542.58|     0|\n",
      "|   3|  15619304|    Onio|        502|  France|Female| 42|     8|   159660.8|          3|        1|           0|113931.57|     1|\n",
      "+----+----------+--------+-----------+--------+------+---+------+-----------+-----------+---------+------------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('bankCustomer.csv', inferSchema = True, header=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SrNo: integer (nullable = true)\n",
      " |-- CustomerId: integer (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- CreditScore: integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Tenure: integer (nullable = true)\n",
      " |-- CurrBalance: double (nullable = true)\n",
      " |-- NumProducts: integer (nullable = true)\n",
      " |-- OwnCrCard: integer (nullable = true)\n",
      " |-- ActiveMember: integer (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      " |-- Exited: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use One hot Encoding for the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "location_indexer = StringIndexer(inputCol=\"Location\", outputCol=\"locationIndex\")\n",
    "sex_indexer = StringIndexer(inputCol=\"Sex\", outputCol=\"genderIndex\")\n",
    "onehotencoder_location_vector = OneHotEncoder(inputCol=\"locationIndex\", outputCol=\"location_vec\")\n",
    "onehotencoder_gender_vector = OneHotEncoder(inputCol=\"genderIndex\", outputCol=\"gender_vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline(stages=[location_indexer,sex_indexer,onehotencoder_location_vector,onehotencoder_gender_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----------+--------+------+---+------+-----------+-----------+---------+------------+---------+------+-------------+-----------+-------------+-------------+\n",
      "|SrNo|CustomerId| LastName|CreditScore|Location|   Sex|Age|Tenure|CurrBalance|NumProducts|OwnCrCard|ActiveMember|   Salary|Exited|locationIndex|genderIndex| location_vec|   gender_vec|\n",
      "+----+----------+---------+-----------+--------+------+---+------+-----------+-----------+---------+------------+---------+------+-------------+-----------+-------------+-------------+\n",
      "|   1|  15634602| Hargrave|        619|  France|Female| 42|     2|        0.0|          1|        1|           1|101348.88|     1|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "|   2|  15647311|     Hill|        608|   Spain|Female| 41|     1|   83807.86|          1|        0|           1|112542.58|     0|          2.0|        1.0|    (2,[],[])|    (1,[],[])|\n",
      "|   3|  15619304|     Onio|        502|  France|Female| 42|     8|   159660.8|          3|        1|           0|113931.57|     1|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "|   4|  15701354|     Boni|        699|  France|Female| 39|     1|        0.0|          2|        0|           0| 93826.63|     0|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "|   5|  15737888| Mitchell|        850|   Spain|Female| 43|     2|  125510.82|          1|        1|           1|  79084.1|     0|          2.0|        1.0|    (2,[],[])|    (1,[],[])|\n",
      "|   6|  15574012|      Chu|        645|   Spain|  Male| 44|     8|  113755.78|          2|        1|           0|149756.71|     1|          2.0|        0.0|    (2,[],[])|(1,[0],[1.0])|\n",
      "|   7|  15592531| Bartlett|        822|  France|  Male| 50|     7|        0.0|          2|        1|           1|  10062.8|     0|          0.0|        0.0|(2,[0],[1.0])|(1,[0],[1.0])|\n",
      "|   8|  15656148|   Obinna|        376| Germany|Female| 29|     4|  115046.74|          4|        1|           0|119346.88|     1|          1.0|        1.0|(2,[1],[1.0])|    (1,[],[])|\n",
      "|   9|  15792365|       He|        501|  France|  Male| 44|     4|  142051.07|          2|        0|           1|  74940.5|     0|          0.0|        0.0|(2,[0],[1.0])|(1,[0],[1.0])|\n",
      "|  10|  15592389|       H?|        684|  France|  Male| 27|     2|  134603.88|          1|        1|           1| 71725.73|     0|          0.0|        0.0|(2,[0],[1.0])|(1,[0],[1.0])|\n",
      "|  11|  15767821|   Bearce|        528|  France|  Male| 31|     6|  102016.72|          2|        0|           0| 80181.12|     0|          0.0|        0.0|(2,[0],[1.0])|(1,[0],[1.0])|\n",
      "|  12|  15737173|  Andrews|        497|   Spain|  Male| 24|     3|        0.0|          2|        1|           0| 76390.01|     0|          2.0|        0.0|    (2,[],[])|(1,[0],[1.0])|\n",
      "|  13|  15632264|      Kay|        476|  France|Female| 34|    10|        0.0|          2|        1|           0| 26260.98|     0|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "|  14|  15691483|     Chin|        549|  France|Female| 25|     5|        0.0|          2|        0|           0|190857.79|     0|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "|  15|  15600882|    Scott|        635|   Spain|Female| 35|     7|        0.0|          2|        1|           1| 65951.65|     0|          2.0|        1.0|    (2,[],[])|    (1,[],[])|\n",
      "|  16|  15643966|  Goforth|        616| Germany|  Male| 45|     3|  143129.41|          2|        0|           1| 64327.26|     0|          1.0|        0.0|(2,[1],[1.0])|(1,[0],[1.0])|\n",
      "|  17|  15737452|    Romeo|        653| Germany|  Male| 58|     1|  132602.88|          1|        1|           0|  5097.67|     1|          1.0|        0.0|(2,[1],[1.0])|(1,[0],[1.0])|\n",
      "|  18|  15788218|Henderson|        549|   Spain|Female| 24|     9|        0.0|          2|        1|           1| 14406.41|     0|          2.0|        1.0|    (2,[],[])|    (1,[],[])|\n",
      "|  19|  15661507|  Muldrow|        587|   Spain|  Male| 45|     6|        0.0|          1|        0|           0|158684.81|     0|          2.0|        0.0|    (2,[],[])|(1,[0],[1.0])|\n",
      "|  20|  15568982|      Hao|        726|  France|Female| 24|     6|        0.0|          2|        1|           1| 54724.03|     0|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "+----+----------+---------+-----------+--------+------+---+------+-----------+-----------+---------+------------+---------+------+-------------+-----------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----------+--------+------+---+------+-----------+-----------+---------+------------+---------+------+-------------+-----------+-------------+-------------+\n",
      "|SrNo|CustomerId| LastName|CreditScore|Location|   Sex|Age|Tenure|CurrBalance|NumProducts|OwnCrCard|ActiveMember|   Salary|Exited|locationIndex|genderIndex| location_vec|   gender_vec|\n",
      "+----+----------+---------+-----------+--------+------+---+------+-----------+-----------+---------+------------+---------+------+-------------+-----------+-------------+-------------+\n",
      "|   1|  15634602| Hargrave|        619|  France|Female| 42|     2|        0.0|          1|        1|           1|101348.88|     1|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "|   2|  15647311|     Hill|        608|   Spain|Female| 41|     1|   83807.86|          1|        0|           1|112542.58|     0|          2.0|        1.0|    (2,[],[])|    (1,[],[])|\n",
      "|   3|  15619304|     Onio|        502|  France|Female| 42|     8|   159660.8|          3|        1|           0|113931.57|     1|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "|   4|  15701354|     Boni|        699|  France|Female| 39|     1|        0.0|          2|        0|           0| 93826.63|     0|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "|   5|  15737888| Mitchell|        850|   Spain|Female| 43|     2|  125510.82|          1|        1|           1|  79084.1|     0|          2.0|        1.0|    (2,[],[])|    (1,[],[])|\n",
      "|   6|  15574012|      Chu|        645|   Spain|  Male| 44|     8|  113755.78|          2|        1|           0|149756.71|     1|          2.0|        0.0|    (2,[],[])|(1,[0],[1.0])|\n",
      "|   7|  15592531| Bartlett|        822|  France|  Male| 50|     7|        0.0|          2|        1|           1|  10062.8|     0|          0.0|        0.0|(2,[0],[1.0])|(1,[0],[1.0])|\n",
      "|   8|  15656148|   Obinna|        376| Germany|Female| 29|     4|  115046.74|          4|        1|           0|119346.88|     1|          1.0|        1.0|(2,[1],[1.0])|    (1,[],[])|\n",
      "|   9|  15792365|       He|        501|  France|  Male| 44|     4|  142051.07|          2|        0|           1|  74940.5|     0|          0.0|        0.0|(2,[0],[1.0])|(1,[0],[1.0])|\n",
      "|  10|  15592389|       H?|        684|  France|  Male| 27|     2|  134603.88|          1|        1|           1| 71725.73|     0|          0.0|        0.0|(2,[0],[1.0])|(1,[0],[1.0])|\n",
      "|  11|  15767821|   Bearce|        528|  France|  Male| 31|     6|  102016.72|          2|        0|           0| 80181.12|     0|          0.0|        0.0|(2,[0],[1.0])|(1,[0],[1.0])|\n",
      "|  12|  15737173|  Andrews|        497|   Spain|  Male| 24|     3|        0.0|          2|        1|           0| 76390.01|     0|          2.0|        0.0|    (2,[],[])|(1,[0],[1.0])|\n",
      "|  13|  15632264|      Kay|        476|  France|Female| 34|    10|        0.0|          2|        1|           0| 26260.98|     0|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "|  14|  15691483|     Chin|        549|  France|Female| 25|     5|        0.0|          2|        0|           0|190857.79|     0|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "|  15|  15600882|    Scott|        635|   Spain|Female| 35|     7|        0.0|          2|        1|           1| 65951.65|     0|          2.0|        1.0|    (2,[],[])|    (1,[],[])|\n",
      "|  16|  15643966|  Goforth|        616| Germany|  Male| 45|     3|  143129.41|          2|        0|           1| 64327.26|     0|          1.0|        0.0|(2,[1],[1.0])|(1,[0],[1.0])|\n",
      "|  17|  15737452|    Romeo|        653| Germany|  Male| 58|     1|  132602.88|          1|        1|           0|  5097.67|     1|          1.0|        0.0|(2,[1],[1.0])|(1,[0],[1.0])|\n",
      "|  18|  15788218|Henderson|        549|   Spain|Female| 24|     9|        0.0|          2|        1|           1| 14406.41|     0|          2.0|        1.0|    (2,[],[])|    (1,[],[])|\n",
      "|  19|  15661507|  Muldrow|        587|   Spain|  Male| 45|     6|        0.0|          1|        0|           0|158684.81|     0|          2.0|        0.0|    (2,[],[])|(1,[0],[1.0])|\n",
      "|  20|  15568982|      Hao|        726|  France|Female| 24|     6|        0.0|          2|        1|           1| 54724.03|     0|          0.0|        1.0|(2,[0],[1.0])|    (1,[],[])|\n",
      "+----+----------+---------+-----------+--------+------+---+------+-----------+-----------+---------+------------+---------+------+-------------+-----------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed=df_transformed.withColumn(\"Exited\",df.Exited)\n",
    "df_transformed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = [\"CreditScore\", \"Age\", \"Tenure\", \"CurrBalance\", \"NumProducts\", \"OwnCrCard\",\"ActiveMember\",\"Salary\",\"location_vec\", \"gender_vec\"]\n",
    "outputCol = \"features\"\n",
    "df_va = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
    "df_final = df_va.transform(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_final.select([\"features\",\"Exited\"])\n",
    "df_final=df_final.withColumnRenamed(\"Exited\",\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7043\n",
      "2957\n"
     ]
    }
   ],
   "source": [
    "train, test = df_final.randomSplit([0.7, 0.3], seed=100)\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the various models and select the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol = 'label', featuresCol = 'features', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(11, {0: 519.0, 1: 47.0, 2: 6.0, 3: 157296.02, 4: 2.0, 7: 147278.43}), label=1, rawPrediction=DenseVector([-0.1547, 0.1547]), probability=DenseVector([0.4614, 0.5386]), prediction=1.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = lrModel.transform(test)\n",
    "predictions.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC 0.770944097119996\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print('Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC 0.7760017087208159\n"
     ]
    }
   ],
   "source": [
    "predictions = cvModel.transform(test)\n",
    "print('Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numNodes =  13\n",
      "depth =  3\n"
     ]
    }
   ],
   "source": [
    "print(\"numNodes = \", dtModel.numNodes)\n",
    "print(\"depth = \", dtModel.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = dtModel.transform(test)\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------+--------------------+----------+\n",
      "|            features|label| rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------+--------------------+----------+\n",
      "|(11,[0,1,2,3,4,7]...|    1| [321.0,290.0]|[0.52536824877250...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    1| [321.0,290.0]|[0.52536824877250...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    0|[4375.0,523.0]|[0.89322172315230...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    0|[4375.0,523.0]|[0.89322172315230...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    0|[4375.0,523.0]|[0.89322172315230...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    0|[4375.0,523.0]|[0.89322172315230...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    0|[4375.0,523.0]|[0.89322172315230...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    0|[4375.0,523.0]|[0.89322172315230...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    1|  [43.0,248.0]|[0.14776632302405...|       1.0|\n",
      "|(11,[0,1,2,3,4,7]...|    0|[4375.0,523.0]|[0.89322172315230...|       0.0|\n",
      "+--------------------+-----+--------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2569322977099677"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [1, 2, 6, 10])\n",
    "             .addGrid(dt.maxBins, [20, 40, 80])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numNodes =  491\n",
      "depth =  10\n"
     ]
    }
   ],
   "source": [
    "print(\"numNodes = \", cvModel.bestModel.numNodes)\n",
    "print(\"depth = \", cvModel.bestModel.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6306572513840061"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+--------------------+----------+\n",
      "|            features|label|rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+-------------+--------------------+----------+\n",
      "|(11,[0,1,2,3,4,7]...|    1|   [23.0,6.0]|[0.79310344827586...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    1| [109.0,11.0]|[0.90833333333333...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    0| [484.0,54.0]|[0.89962825278810...|       0.0|\n",
      "+--------------------+-----+-------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(11,[0,1,2,3,4,7]...|    1|[11.7802956554716...|[0.58901478277358...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    1|[11.9574743961301...|[0.59787371980650...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    0|[17.4434786554100...|[0.87217393277050...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8457446501451825"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.maxBins, [20, 60])\n",
    "             .addGrid(rf.numTrees, [5, 20])\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\n",
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8543405694196703"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(11,[0,1,2,3,4,7]...|    1|[11.6367825390349...|[0.58183912695174...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    1|[13.5564240323507...|[0.67782120161753...|       0.0|\n",
      "|(11,[0,1,2,3,4,7]...|    0|[16.8810018565629...|[0.84405009282814...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8586116347754902"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel = cvModel.bestModel\n",
    "final_predictions = bestModel.transform(df_final)\n",
    "evaluator.evaluate(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
